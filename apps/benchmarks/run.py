#!/usr/bin/env python3
"""
ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ - ê°„ë‹¨í•œ ë˜í¼

ì‚¬ìš©ë²•:
    # ê°€ì¥ ê°„ë‹¨ (Semanticaë§Œ)
    python apps/benchmarks/run.py

    # Codyì™€ ë¹„êµ
    python apps/benchmarks/run.py --with-cody

    # ì»¤ìŠ¤í…€ ì¿¼ë¦¬
    python apps/benchmarks/run.py --queries my_queries.txt
"""

import json
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from apps.benchmarks.evaluators.metrics import GroundTruth, MetricsCalculator
from apps.benchmarks.evaluators.semantica import SemanticaEvaluator

try:
    from rich.console import Console
    from rich.progress import Progress, SpinnerColumn, TextColumn, TimeElapsedColumn
    from rich.table import Table

    HAS_RICH = True
except ImportError:
    HAS_RICH = False


@dataclass
class QueryAnalysis:
    """ì¿¼ë¦¬ë³„ ìƒì„¸ ë¶„ì„"""

    query: str
    precision: float
    recall: float
    reciprocal_rank: float
    expected_count: int
    found_count: int
    missing_items: list[str]
    unexpected_items: list[str]


def analyze_query(result, ground_truth: GroundTruth, k: int) -> QueryAnalysis:
    """ì¿¼ë¦¬ë³„ ìƒì„¸ ë¶„ì„ ìƒì„±"""
    top_k = set(result.results[:k])
    relevant = top_k & ground_truth.relevant_items
    missing = ground_truth.relevant_items - top_k
    unexpected = top_k - ground_truth.relevant_items

    precision = MetricsCalculator.precision_at_k(result, ground_truth, k)
    recall = MetricsCalculator.recall_at_k(result, ground_truth, k)
    rr = MetricsCalculator.reciprocal_rank(result, ground_truth)

    return QueryAnalysis(
        query=result.query,
        precision=precision,
        recall=recall,
        reciprocal_rank=rr,
        expected_count=len(ground_truth.relevant_items),
        found_count=len(relevant),
        missing_items=sorted(missing),
        unexpected_items=sorted(unexpected),
    )


def simple_benchmark():
    """ê°€ì¥ ê°„ë‹¨í•œ ë²¤ì¹˜ë§ˆí¬ - ëŒ€í™”í˜•"""
    print("=" * 80)
    print("ğŸš€ Semantica ë²¤ì¹˜ë§ˆí¬")
    print("=" * 80)
    print()

    # ì €ì¥ì†Œ ID
    print("ğŸ“ ì €ì¥ì†Œ ì„¤ì •")
    repo_id = input("  ì €ì¥ì†Œ ID (ê¸°ë³¸: semantica-codegraph): ").strip()
    if not repo_id:
        repo_id = "semantica-codegraph"
    print(f"  âœ“ ì €ì¥ì†Œ: {repo_id}")
    print()

    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì„ íƒ
    print("ğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì„ íƒ")
    print("  1. ê¸°ë³¸ ì¿¼ë¦¬ 5ê°œ (ë¹ ë¦„)")
    print("  2. ì „ì²´ ì¿¼ë¦¬ 10ê°œ (ìƒì„¸)")
    print("  3. ì§ì ‘ ì…ë ¥")
    choice = input("  ì„ íƒ (1-3, ê¸°ë³¸: 1): ").strip() or "1"
    print()

    if choice == "1":
        queries = ["ì„¤ì • íŒŒì¼", "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°", "ê²€ìƒ‰ êµ¬í˜„", "íŒŒì„œ", "í…ŒìŠ¤íŠ¸"]
        print(f"  âœ“ ê¸°ë³¸ ì¿¼ë¦¬ {len(queries)}ê°œ ì‚¬ìš©")
    elif choice == "2":
        queries_file = project_root / "apps/benchmarks/datasets/semantica_queries.txt"
        if queries_file.exists():
            with queries_file.open() as f:
                queries = [line.strip() for line in f if line.strip()]
            print(f"  âœ“ ì „ì²´ ì¿¼ë¦¬ {len(queries)}ê°œ ì‚¬ìš©")
        else:
            print("  âœ— ì¿¼ë¦¬ íŒŒì¼ ì—†ìŒ, ê¸°ë³¸ ì¿¼ë¦¬ ì‚¬ìš©")
            queries = ["ì„¤ì •", "ê²€ìƒ‰", "íŒŒì„œ"]
    else:
        print("  ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ë¹ˆ ì¤„ë¡œ ì¢…ë£Œ):")
        queries = []
        while True:
            q = input("    > ").strip()
            if not q:
                break
            queries.append(q)
        print(f"  âœ“ ì¿¼ë¦¬ {len(queries)}ê°œ ì…ë ¥ë¨")

    if not queries:
        print("  âœ— ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print()
    print("ğŸ”§ Semantica ì´ˆê¸°í™” ì¤‘...")
    try:
        evaluator = SemanticaEvaluator()
        print("  âœ“ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"  âœ— ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print()
        print("ğŸ’¡ ë¬¸ì œ í•´ê²°:")
        print("  - PostgreSQLì´ ì‹¤í–‰ ì¤‘ì¸ê°€ìš”? (docker-compose up -d)")
        print("  - MeiliSearchê°€ ì‹¤í–‰ ì¤‘ì¸ê°€ìš”?")
        print("  - .env íŒŒì¼ì´ ì„¤ì •ë˜ì–´ ìˆë‚˜ìš”?")
        return

    print()
    print("=" * 80)
    print("ğŸ” ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
    print("=" * 80)
    print()

    k = 3
    results = []

    for i, query in enumerate(queries, 1):
        print(f"[{i}/{len(queries)}] '{query}'")
        try:
            result = evaluator.search(repo_id, query, k)
            results.append(result)

            print(f"  â±ï¸  {result.latency_ms:.1f}ms")
            if result.results:
                for j, path in enumerate(result.results[:k], 1):
                    print(f"    {j}. {path}")
            else:
                print("    (ê²°ê³¼ ì—†ìŒ)")
        except Exception as e:
            print(f"  âœ— ì—ëŸ¬: {e}")
        print()

    # ê°„ë‹¨í•œ í†µê³„
    if results:
        avg_latency = sum(r.latency_ms for r in results) / len(results)
        print("=" * 80)
        print("ğŸ“Š í†µê³„")
        print("=" * 80)
        print(f"ì´ ì¿¼ë¦¬:      {len(results)}ê°œ")
        print(f"í‰ê·  ì‘ë‹µ:    {avg_latency:.1f}ms")
        print(f"ê°€ì¥ ë¹ ë¦„:    {min(r.latency_ms for r in results):.1f}ms")
        print(f"ê°€ì¥ ëŠë¦¼:    {max(r.latency_ms for r in results):.1f}ms")

        # ê²°ê³¼ í’ˆì§ˆ ê°„ë‹¨ ì²´í¬
        total_results = sum(len(r.results) for r in results)
        print(f"ì´ ê²°ê³¼:      {total_results}ê°œ")
        print()

        # ì„±ëŠ¥ í‰ê°€
        if avg_latency < 200:
            print("âœ… ì‘ë‹µ ì†ë„: ë¹ ë¦„ (200ms ë¯¸ë§Œ)")
        elif avg_latency < 500:
            print("âš ï¸  ì‘ë‹µ ì†ë„: ë³´í†µ (200-500ms)")
        else:
            print("âŒ ì‘ë‹µ ì†ë„: ëŠë¦¼ (500ms ì´ìƒ)")

    print()
    print("=" * 80)
    print()
    print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("  - ì •ë‹µ ë°ì´í„°ë¡œ ì •í™•ë„ í‰ê°€: python apps/benchmarks/run.py --evaluate")
    print("  - Codyì™€ ë¹„êµ: python apps/benchmarks/run.py --with-cody")
    print("  - ì»¤ìŠ¤í…€ ì¿¼ë¦¬: python apps/benchmarks/run.py --queries my_queries.txt")
    print()


def full_evaluation(repo_id: str, save_results: bool = True):
    """ì •ë‹µ ë°ì´í„°ë¡œ ì™„ì „í•œ í‰ê°€"""
    if HAS_RICH:
        console = Console()
        console.print("[bold cyan]ğŸ“Š ì •í™•ë„ í‰ê°€[/bold cyan]")
        console.print()
    else:
        print("=" * 80)
        print("ğŸ“Š ì •í™•ë„ í‰ê°€")
        print("=" * 80)
        print()

    # ì •ë‹µ ë°ì´í„° ë¡œë“œ
    gt_file = Path(__file__).parent / "datasets/semantica_ground_truth.json"
    if not gt_file.exists():
        print(f"âœ— ì •ë‹µ ë°ì´í„° ì—†ìŒ: {gt_file}")
        print()
        print("ğŸ’¡ ì •ë‹µ ë°ì´í„° ìƒì„±:")
        print("  apps/benchmarks/datasets/semantica_ground_truth.json íŒŒì¼ì„ ì‘ì„±í•˜ì„¸ìš”.")
        return

    with gt_file.open() as f:
        gt_data = json.load(f)

    ground_truths = [GroundTruth(item["query"], set(item["relevant_items"])) for item in gt_data]

    k = 5

    if HAS_RICH:
        console.print(f"ğŸ“ ì¿¼ë¦¬: {len(ground_truths)}ê°œ")
        console.print(f"ğŸ“ ì €ì¥ì†Œ: {repo_id}")
        console.print(f"ğŸ”¢ K: {k}")
        console.print()
    else:
        print(f"ğŸ“ ì¿¼ë¦¬: {len(ground_truths)}ê°œ")
        print(f"ğŸ“ ì €ì¥ì†Œ: {repo_id}")
        print(f"ğŸ”¢ K: {k}")
        print()

    # í‰ê°€ ì‹¤í–‰
    print("ğŸ”§ ì´ˆê¸°í™” ì¤‘...")
    evaluator = SemanticaEvaluator()

    # ê²€ìƒ‰ ì‹¤í–‰ (í”„ë¡œê·¸ë ˆìŠ¤ ë°” í¬í•¨)
    results = []
    query_analyses = []

    if HAS_RICH:
        with Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            console=console,
        ) as progress:
            task = progress.add_task("ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...", total=len(ground_truths))

            for gt in ground_truths:
                result = evaluator.search(repo_id, gt.query, k)
                results.append(result)

                # ì¿¼ë¦¬ë³„ ë¶„ì„
                analysis = analyze_query(result, gt, k)
                query_analyses.append(analysis)

                progress.update(task, advance=1)
    else:
        print("ğŸ” ê²€ìƒ‰ ì¤‘...")
        for i, gt in enumerate(ground_truths, 1):
            result = evaluator.search(repo_id, gt.query, k)
            results.append(result)

            analysis = analyze_query(result, gt, k)
            query_analyses.append(analysis)

            print(f"  [{i}/{len(ground_truths)}] {gt.query[:40]}...")

    # ì „ì²´ ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics = MetricsCalculator.evaluate_batch(results, ground_truths, k)

    print()

    # ê²°ê³¼ ì¶œë ¥
    if HAS_RICH:
        _print_results_rich(console, metrics, query_analyses)
    else:
        _print_results_plain(metrics, query_analyses)

    # ê²°ê³¼ ì €ì¥
    if save_results:
        output_dir = project_root / ".temp/benchmark_results"
        _save_results(metrics, query_analyses, repo_id, output_dir)

    # ì¢…í•© í‰ê°€
    score = 0
    if metrics.precision_at_k > 0.6:
        score += 1
    if metrics.recall_at_k > 0.5:
        score += 1
    if metrics.mrr > 0.7:
        score += 1
    if metrics.avg_latency_ms < 200:
        score += 1

    return score, metrics, query_analyses


def _print_results_rich(console: Console, metrics, analyses):
    """Rich ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ê²°ê³¼ ì¶œë ¥"""
    # ì¿¼ë¦¬ë³„ ê²°ê³¼ í…Œì´ë¸”
    table = Table(title="ì¿¼ë¦¬ë³„ í‰ê°€ ê²°ê³¼", show_lines=True)
    table.add_column("ì¿¼ë¦¬", style="cyan", width=35)
    table.add_column("Prec", justify="right", style="green")
    table.add_column("Recall", justify="right", style="yellow")
    table.add_column("RR", justify="right", style="magenta")
    table.add_column("ë§¤ì¹­", justify="center")
    table.add_column("ìƒíƒœ", justify="center")

    for a in analyses:
        query_display = a.query[:32] + "..." if len(a.query) > 35 else a.query

        if a.recall >= 0.7:
            status, style = "âœ…", "green"
        elif a.recall >= 0.5:
            status, style = "âš ï¸", "yellow"
        else:
            status, style = "âŒ", "red"

        matching = f"{a.found_count}/{a.expected_count}"

        table.add_row(
            query_display,
            f"{a.precision:.2f}",
            f"{a.recall:.2f}",
            f"{a.reciprocal_rank:.2f}",
            matching,
            status,
            style=style,
        )

    console.print(table)
    console.print()

    # ì „ì²´ ë©”íŠ¸ë¦­ í…Œì´ë¸”
    metrics_table = Table(title="ì „ì²´ ë©”íŠ¸ë¦­", show_header=False)
    metrics_table.add_column("ë©”íŠ¸ë¦­", style="cyan bold", width=20)
    metrics_table.add_column("ê°’", style="white", width=15)
    metrics_table.add_column("ê¸°ì¤€", style="dim", width=20)
    metrics_table.add_column("í‰ê°€", justify="center")

    metrics_data = [
        (
            "Precision@K",
            f"{metrics.precision_at_k:.3f}",
            "> 0.6 (Good)",
            "âœ…" if metrics.precision_at_k > 0.6 else "âŒ",
        ),
        (
            "Recall@K",
            f"{metrics.recall_at_k:.3f}",
            "> 0.5 (Good)",
            "âœ…" if metrics.recall_at_k > 0.5 else "âŒ",
        ),
        ("MRR", f"{metrics.mrr:.3f}", "> 0.7 (Good)", "âœ…" if metrics.mrr > 0.7 else "âŒ"),
        (
            "Avg Latency",
            f"{metrics.avg_latency_ms:.1f}ms",
            "< 200ms (Fast)",
            "âœ…" if metrics.avg_latency_ms < 200 else "âš ï¸",
        ),
    ]

    for name, value, threshold, status in metrics_data:
        metrics_table.add_row(name, value, threshold, status)

    console.print(metrics_table)

    # ì‹¤íŒ¨ ì¼€ì´ìŠ¤
    failed = [a for a in analyses if a.recall < 0.5]
    if failed:
        console.print()
        console.print("[bold red]ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„[/bold red] (Recall < 0.5)\n")

        for i, a in enumerate(failed, 1):
            console.print(f"[bold]{i}. {a.query}[/bold]")
            console.print(f"   Recall: {a.recall:.2f}")

            if a.missing_items:
                console.print("   [red]ëˆ„ë½:[/red]")
                for item in a.missing_items[:2]:
                    console.print(f"     - {item}")
                if len(a.missing_items) > 2:
                    console.print(f"     ... ì™¸ {len(a.missing_items) - 2}ê°œ")
            console.print()


def _print_results_plain(metrics, analyses):
    """Plain í…ìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥"""
    print("=" * 80)
    print("ê²°ê³¼")
    print("=" * 80)
    print(metrics)
    print()

    # í‰ê°€
    if metrics.precision_at_k > 0.6:
        print("âœ… Precision: ì¢‹ìŒ")
    else:
        print("âŒ Precision: ê°œì„  í•„ìš”")

    if metrics.recall_at_k > 0.5:
        print("âœ… Recall: ì¢‹ìŒ")
    else:
        print("âŒ Recall: ê°œì„  í•„ìš”")

    if metrics.mrr > 0.7:
        print("âœ… MRR: ì¢‹ìŒ")
    else:
        print("âŒ MRR: ê°œì„  í•„ìš”")

    if metrics.avg_latency_ms < 200:
        print("âœ… Latency: ë¹ ë¦„")
    else:
        print("âš ï¸  Latency: ë³´í†µ")

    print()

    # ì‹¤íŒ¨ ì¼€ì´ìŠ¤
    failed = [a for a in analyses if a.recall < 0.5]
    if failed:
        print(f"ì‹¤íŒ¨ ì¼€ì´ìŠ¤: {len(failed)}ê°œ")
        for a in failed:
            print(f"  - {a.query} (Recall: {a.recall:.2f})")
        print()


def _save_results(metrics, analyses, repo_id, output_dir):
    """ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now()
    run_data = {
        "timestamp": timestamp.isoformat(),
        "repo_id": repo_id,
        "summary": {
            "precision_at_k": metrics.precision_at_k,
            "recall_at_k": metrics.recall_at_k,
            "mrr": metrics.mrr,
            "avg_latency_ms": metrics.avg_latency_ms,
            "total_queries": metrics.total_queries,
        },
        "queries": [
            {
                "query": a.query,
                "precision": a.precision,
                "recall": a.recall,
                "reciprocal_rank": a.reciprocal_rank,
                "found": a.found_count,
                "expected": a.expected_count,
                "missing": a.missing_items,
                "unexpected": a.unexpected_items,
            }
            for a in analyses
        ],
    }

    filename = f"benchmark_{timestamp.strftime('%Y%m%d_%H%M%S')}.json"
    output_file = output_dir / filename

    with output_file.open("w", encoding="utf-8") as f:
        json.dump(run_data, f, indent=2, ensure_ascii=False)

    if HAS_RICH:
        console = Console()
        console.print(f"[green]âœ… ê²°ê³¼ ì €ì¥:[/green] {output_file}")
    else:
        print(f"âœ… ê²°ê³¼ ì €ì¥: {output_file}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="ê°„ë‹¨í•œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
    parser.add_argument("--evaluate", action="store_true", help="ì •ë‹µ ë°ì´í„°ë¡œ ì •í™•ë„ í‰ê°€")
    parser.add_argument("--with-cody", action="store_true", help="Codyì™€ ë¹„êµ")
    parser.add_argument("--repo-id", help="ì €ì¥ì†Œ ID")
    parser.add_argument("--queries", help="ì¿¼ë¦¬ íŒŒì¼ ê²½ë¡œ")

    args = parser.parse_args()

    if args.evaluate:
        repo_id = args.repo_id or "semantica-codegraph"
        result = full_evaluation(repo_id)
        if result:
            score, metrics, analyses = result
            print()
            print(f"ì¢…í•© ì ìˆ˜: {score}/4")
            if score >= 3:
                print("âœ… ì „ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥!")
            else:
                print("âš ï¸  ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print()
    elif args.with_cody:
        print("Cody ë¹„êµëŠ” ì•„ì§ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.")
        print("ì„ì‹œë¡œ compare.pyë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:")
        print("  python -m apps.benchmarks.compare --interactive")
    else:
        simple_benchmark()


if __name__ == "__main__":
    main()
