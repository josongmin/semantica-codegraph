#!/usr/bin/env python3
"""
ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ - ê°„ë‹¨í•œ ë˜í¼

ì‚¬ìš©ë²•:
    # ê°€ì¥ ê°„ë‹¨ (Semanticaë§Œ)
    python benchmarks/run.py
    
    # Codyì™€ ë¹„êµ
    python benchmarks/run.py --with-cody
    
    # ì»¤ìŠ¤í…€ ì¿¼ë¦¬
    python benchmarks/run.py --queries my_queries.txt
"""

import sys
import os
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from benchmarks.evaluators.semantica import SemanticaEvaluator
from benchmarks.evaluators.metrics import GroundTruth, MetricsCalculator


def simple_benchmark():
    """ê°€ì¥ ê°„ë‹¨í•œ ë²¤ì¹˜ë§ˆí¬ - ëŒ€í™”í˜•"""
    print("=" * 80)
    print("ğŸš€ Semantica ë²¤ì¹˜ë§ˆí¬")
    print("=" * 80)
    print()
    
    # ì €ì¥ì†Œ ID
    print("ğŸ“ ì €ì¥ì†Œ ì„¤ì •")
    repo_id = input("  ì €ì¥ì†Œ ID (ê¸°ë³¸: semantica-codegraph): ").strip()
    if not repo_id:
        repo_id = "semantica-codegraph"
    print(f"  âœ“ ì €ì¥ì†Œ: {repo_id}")
    print()
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì„ íƒ
    print("ğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì„ íƒ")
    print("  1. ê¸°ë³¸ ì¿¼ë¦¬ 5ê°œ (ë¹ ë¦„)")
    print("  2. ì „ì²´ ì¿¼ë¦¬ 10ê°œ (ìƒì„¸)")
    print("  3. ì§ì ‘ ì…ë ¥")
    choice = input("  ì„ íƒ (1-3, ê¸°ë³¸: 1): ").strip() or "1"
    print()
    
    if choice == "1":
        queries = [
            "ì„¤ì • íŒŒì¼",
            "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°",
            "ê²€ìƒ‰ êµ¬í˜„",
            "íŒŒì„œ",
            "í…ŒìŠ¤íŠ¸"
        ]
        print(f"  âœ“ ê¸°ë³¸ ì¿¼ë¦¬ {len(queries)}ê°œ ì‚¬ìš©")
    elif choice == "2":
        queries_file = project_root / "benchmarks/datasets/semantica_queries.txt"
        if queries_file.exists():
            with open(queries_file) as f:
                queries = [line.strip() for line in f if line.strip()]
            print(f"  âœ“ ì „ì²´ ì¿¼ë¦¬ {len(queries)}ê°œ ì‚¬ìš©")
        else:
            print("  âœ— ì¿¼ë¦¬ íŒŒì¼ ì—†ìŒ, ê¸°ë³¸ ì¿¼ë¦¬ ì‚¬ìš©")
            queries = ["ì„¤ì •", "ê²€ìƒ‰", "íŒŒì„œ"]
    else:
        print("  ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ë¹ˆ ì¤„ë¡œ ì¢…ë£Œ):")
        queries = []
        while True:
            q = input("    > ").strip()
            if not q:
                break
            queries.append(q)
        print(f"  âœ“ ì¿¼ë¦¬ {len(queries)}ê°œ ì…ë ¥ë¨")
    
    if not queries:
        print("  âœ— ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print()
    print("ğŸ”§ Semantica ì´ˆê¸°í™” ì¤‘...")
    try:
        evaluator = SemanticaEvaluator()
        print("  âœ“ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"  âœ— ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print()
        print("ğŸ’¡ ë¬¸ì œ í•´ê²°:")
        print("  - PostgreSQLì´ ì‹¤í–‰ ì¤‘ì¸ê°€ìš”? (docker-compose up -d)")
        print("  - MeiliSearchê°€ ì‹¤í–‰ ì¤‘ì¸ê°€ìš”?")
        print("  - .env íŒŒì¼ì´ ì„¤ì •ë˜ì–´ ìˆë‚˜ìš”?")
        return
    
    print()
    print("=" * 80)
    print("ğŸ” ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")
    print("=" * 80)
    print()
    
    k = 3
    results = []
    
    for i, query in enumerate(queries, 1):
        print(f"[{i}/{len(queries)}] '{query}'")
        try:
            result = evaluator.search(repo_id, query, k)
            results.append(result)
            
            print(f"  â±ï¸  {result.latency_ms:.1f}ms")
            if result.results:
                for j, path in enumerate(result.results[:k], 1):
                    print(f"    {j}. {path}")
            else:
                print("    (ê²°ê³¼ ì—†ìŒ)")
        except Exception as e:
            print(f"  âœ— ì—ëŸ¬: {e}")
        print()
    
    # ê°„ë‹¨í•œ í†µê³„
    if results:
        avg_latency = sum(r.latency_ms for r in results) / len(results)
        print("=" * 80)
        print("ğŸ“Š í†µê³„")
        print("=" * 80)
        print(f"ì´ ì¿¼ë¦¬:      {len(results)}ê°œ")
        print(f"í‰ê·  ì‘ë‹µ:    {avg_latency:.1f}ms")
        print(f"ê°€ì¥ ë¹ ë¦„:    {min(r.latency_ms for r in results):.1f}ms")
        print(f"ê°€ì¥ ëŠë¦¼:    {max(r.latency_ms for r in results):.1f}ms")
        
        # ê²°ê³¼ í’ˆì§ˆ ê°„ë‹¨ ì²´í¬
        total_results = sum(len(r.results) for r in results)
        print(f"ì´ ê²°ê³¼:      {total_results}ê°œ")
        print()
        
        # ì„±ëŠ¥ í‰ê°€
        if avg_latency < 200:
            print("âœ… ì‘ë‹µ ì†ë„: ë¹ ë¦„ (200ms ë¯¸ë§Œ)")
        elif avg_latency < 500:
            print("âš ï¸  ì‘ë‹µ ì†ë„: ë³´í†µ (200-500ms)")
        else:
            print("âŒ ì‘ë‹µ ì†ë„: ëŠë¦¼ (500ms ì´ìƒ)")
    
    print()
    print("=" * 80)
    print()
    print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("  - ì •ë‹µ ë°ì´í„°ë¡œ ì •í™•ë„ í‰ê°€: python benchmarks/run.py --evaluate")
    print("  - Codyì™€ ë¹„êµ: python benchmarks/run.py --with-cody")
    print("  - ì»¤ìŠ¤í…€ ì¿¼ë¦¬: python benchmarks/run.py --queries my_queries.txt")
    print()


def full_evaluation(repo_id: str):
    """ì •ë‹µ ë°ì´í„°ë¡œ ì™„ì „í•œ í‰ê°€"""
    print("=" * 80)
    print("ğŸ“Š ì •í™•ë„ í‰ê°€")
    print("=" * 80)
    print()
    
    # ì •ë‹µ ë°ì´í„° ë¡œë“œ
    gt_file = Path(__file__).parent / "datasets/semantica_ground_truth.json"
    if not gt_file.exists():
        print(f"âœ— ì •ë‹µ ë°ì´í„° ì—†ìŒ: {gt_file}")
        print()
        print("ğŸ’¡ ì •ë‹µ ë°ì´í„° ìƒì„±:")
        print("  benchmarks/datasets/semantica_ground_truth.json íŒŒì¼ì„ ì‘ì„±í•˜ì„¸ìš”.")
        return
    
    import json
    with open(gt_file) as f:
        gt_data = json.load(f)
    
    ground_truths = [
        GroundTruth(item["query"], set(item["relevant_items"]))
        for item in gt_data
    ]
    
    queries = [gt.query for gt in ground_truths]
    
    print(f"ğŸ“ ì¿¼ë¦¬: {len(queries)}ê°œ")
    print(f"ğŸ“ ì €ì¥ì†Œ: {repo_id}")
    print()
    
    # í‰ê°€ ì‹¤í–‰
    print("ğŸ”§ ì´ˆê¸°í™” ì¤‘...")
    evaluator = SemanticaEvaluator()
    
    print("ğŸ” ê²€ìƒ‰ ì¤‘...")
    results = evaluator.batch_search(repo_id, queries, k=5)
    
    print("ğŸ“Š í‰ê°€ ì¤‘...")
    metrics = MetricsCalculator.evaluate_batch(results, ground_truths, k=5)
    
    print()
    print("=" * 80)
    print("ê²°ê³¼")
    print("=" * 80)
    print(metrics)
    print()
    
    # í‰ê°€
    score = 0
    if metrics.precision_at_k > 0.6:
        print("âœ… Precision: ì¢‹ìŒ")
        score += 1
    else:
        print("âŒ Precision: ê°œì„  í•„ìš”")
    
    if metrics.recall_at_k > 0.5:
        print("âœ… Recall: ì¢‹ìŒ")
        score += 1
    else:
        print("âŒ Recall: ê°œì„  í•„ìš”")
    
    if metrics.mrr > 0.7:
        print("âœ… MRR: ì¢‹ìŒ")
        score += 1
    else:
        print("âŒ MRR: ê°œì„  í•„ìš”")
    
    if metrics.avg_latency_ms < 200:
        print("âœ… Latency: ë¹ ë¦„")
        score += 1
    else:
        print("âš ï¸  Latency: ë³´í†µ")
    
    print()
    print(f"ì¢…í•© ì ìˆ˜: {score}/4")
    print()


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="ê°„ë‹¨í•œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
    parser.add_argument("--evaluate", action="store_true", help="ì •ë‹µ ë°ì´í„°ë¡œ ì •í™•ë„ í‰ê°€")
    parser.add_argument("--with-cody", action="store_true", help="Codyì™€ ë¹„êµ")
    parser.add_argument("--repo-id", help="ì €ì¥ì†Œ ID")
    parser.add_argument("--queries", help="ì¿¼ë¦¬ íŒŒì¼ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    if args.evaluate:
        repo_id = args.repo_id or "semantica-codegraph"
        full_evaluation(repo_id)
    elif args.with_cody:
        print("Cody ë¹„êµëŠ” ì•„ì§ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.")
        print("ì„ì‹œë¡œ compare.pyë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:")
        print("  python -m benchmarks.compare --interactive")
    else:
        simple_benchmark()


if __name__ == "__main__":
    main()

